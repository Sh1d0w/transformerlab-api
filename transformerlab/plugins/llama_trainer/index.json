{
  "name": "Huggingface TRL Trainer",
  "uniqueId": "llama_trainer",
  "description": "A training script adapted from https://www.philschmid.de/instruction-tune-llama-2 for training Llama2 using PeFT",
  "plugin-format": "python",
  "type": "trainer",
  "version": "1.0.22",
  "model_architectures": [
    "LlamaForCausalLM",
    "Qwen2ForCausalLM",
    "GemmaForCausalLM",
    "Gemma2ForCausalLM",
    "Gemma3ForCausalLM",
    "Gemma3ForConditionalGeneration",
    "GraniteForCausalLM",
    "AprielForCausalLM",
    "ExaoneForCausalLM",
    "PhiForCausalLM",
    "Phi3ForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM"
  ],
  "git": "",
  "url": "",
  "files": [
    "main.py",
    "setup.sh"
  ],
  "supported_hardware_architectures": [
    "cuda"
  ],
  "setup-script": "setup.sh",
  "parameters": {
    "maximum_sequence_length": {
      "title": "Maximum Sequence Length",
      "type": "integer",
      "default": 2048,
      "minimum": 1,
      "maximum": 4096
    },
    "batch_size": {
      "title": "Batch Size",
      "type": "integer",
      "default": 4,
      "minimum": 1,
      "maximum": 64
    },
    "learning_rate_schedule": {
      "title": "Learning Rate Schedule",
      "type": "string",
      "enum": [
        "constant",
        "linear",
        "cosine",
        "constant_with_warmup"
      ],
      "default": "constant"
    },
    "learning_rate": {
      "title": "Learning Rate",
      "type": "number",
      "default": 5e-5,
      "minimum": 1e-6,
      "maximum": 1e6
    },
    "num_train_epochs": {
      "title": "Number of Training Epochs",
      "type": "integer",
      "default": 1,
      "minimum": 1,
      "maximum": 24
    },
    "max_steps": {
      "title": "Max Steps (-1 means no limit)",
      "type": "integer",
      "default": -1
    },
    "lora_r": {
      "title": "Lora R",
      "type": "number",
      "minimum": 4,
      "maximum": 64,
      "multipleOf": 4,
      "default": 16
    },
    "lora_alpha": {
      "title": "Lora Alpha",
      "type": "number",
      "minimum": 4,
      "maximum": 128,
      "multipleOf": 4,
      "default": 32
    },
    "lora_dropout": {
      "title": "Lora Dropout",
      "type": "number",
      "minimum": 0.05,
      "maximum": 0.9,
      "default": 0.05
    },
    "adaptor_name": {
      "title": "Adaptor Name",
      "type": "string",
      "required": true
    },
    "log_to_wandb": {
      "title": "Log to Weights and Biases",
      "type": "boolean",
      "default": true,
      "required": true
    },
    "fuse_model": {
      "title": "Fuse Model",
      "type": "boolean",
      "default": false
    },
    "run_sweeps": {
      "title": "Run Hyperparameter Sweep",
      "type": "boolean",
      "default": false
    },
    "save_sweep_models": {
      "title": "Save Intermediate Sweep Models",
      "type": "boolean",
      "default": false
    },
    "sweep_metric": {
      "title": "Sweep Optimization Metric",
      "type": "string",
      "default": "eval/loss",
      "enum": ["eval/loss"]
    },
    "lower_is_better": {
      "title": "Lower Values Are Better (For the Sweep Metric)",
      "type": "boolean",
      "default": true
    },
    "train_final_model": {
      "title": "Train Final Model with Best Config",
      "type": "boolean",
      "default": true
    }
  },
  "parameters_ui": {
    "maximum_sequence_length": {
      "ui:help": "Maximum sequence length for the model. Longer sequences will be truncated. Keep lower to save memory."
    },
    "batch_size": {
      "ui:help": "The number of sequences processed simultaneously during training. Higher values lower number of iterations but require more memory."
    },
    "lora_r": {
      "ui:widget": "range",
      "ui:help": "Rank of the update matrices, expressed in int. Lower rank results in smaller update matrices with fewer trainable parameters."
    },
    "lora_alpha": {
      "ui:widget": "range",
      "ui:help": "LoRA scaling factor. Make it a multiple of LoRA R."
    },
    "log_to_wandb": {
      "ui:help": "Log training to Weights and Biases. You must have a Weights and Biases account and API key to use this feature. You need to set the API Key in settings to use this feature."
    },
    "fuse_model": {
      "ui:help": "This will create a new fused model with the adaptor and the model merged. A separate entry will be created in the model zoo for the fused model."
    },
    "run_sweeps": {
      "ui:help": "Run a hyperparameter sweep to find the best configuration before training the final model."
    },
    "save_sweep_models": {
      "ui:help": "Save models from each sweep configuration. Warning: This can use a lot of disk space."
    },
    "sweep_metric": {
      "ui:help": "The metric to optimize during the sweep.",
      "ui:widget": "AutoCompleteWidget",
      "ui:options": {
        "_multiple": false
      }
    },
    "lower_is_better": {
      "ui:help": "Whether lower values of the sweep metric are better (e.g., for loss) or higher values are better (e.g., for accuracy)."
    },
    "train_final_model": {
      "ui:help": "Train a final model using the best configuration found during the sweep."
    }
  }
}