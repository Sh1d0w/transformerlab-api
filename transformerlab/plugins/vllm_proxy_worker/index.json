{
  "name": "vLLM Proxy Server",
  "uniqueId": "vllm_proxy_worker",
  "description": "vLLM is a fast and easy-to-use library for LLM inference and serving.",
  "plugin-format": "python",
  "type": "loader",
  "version": "0.0.1",
  "model_architectures": [
    "CohereForCausalLM",
    "FalconForCausalLM",
    "GemmaForCausalLM",
    "GPTBigCodeForCausalLM",
    "LlamaForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "PhiForCausalLM",
    "Phi3ForCausalLM",
    "Qwen2ForCausalLM"
  ],
  "supported_hardware_architectures": ["cuda"],
  "files": ["main.py", "setup.sh"],
  "setup-script": "setup.sh",
  "parameters": {
    "max_model_len": {
      "title": "Maximum Model Length",
      "type": "integer"
    },
    "gpu_memory_utilization": {
      "title": "GPU Memory Utilization",
      "type": "number",
      "default": 0.9,
      "minimum": 0.0,
      "maximum": 1.0
    },
    "model_dtype": {
      "title": "Select a specific data type for the model",
      "type": "string",
      "enum": ["auto", "float16", "bfloat16", "float32"]
    },
    "port": {
      "title": "Port on which the vLLM server will run",
      "type": "integer",
      "default": 8000
    }
    },
    "parameters_ui": {
    "max_model_len": {
    },
    "gpu_memory_utilization": {
    },
    "model_dtype": {
    },
    "port": {
    }   
  }
  }
