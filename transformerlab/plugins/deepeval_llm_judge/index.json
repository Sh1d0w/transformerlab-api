{
    "name": "DeepEval Evaluations (LLM-as-Judge)",
    "uniqueId": "deepeval_llm_judge",
    "description": "Using LLMs as Judges for evaluating outputs of other LLMs",
    "plugin-format": "python",
    "type": "evaluator",
    "version": "0.1.0",
    "git": "https://github.com/confident-ai/deepeval",
    "url": "https://github.com/confident-ai/deepeval",
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": {
        "task": {
            "title": "Evaluation Metric",
            "type": "string",
            "enum": [
                "Answer Relevancy",
                "FaithFulness",
                "Hallucination",
                "Bias",
                "Toxicity",
                "Contextual Precision",
                "Contextual Recall",
                "Contextual Relevancy"
            ]
        },
        "dataset_path": {
            "title": "Dataset Path",
            "type": "string",
            "format": "data-url"
        }
    },
    "parameters-ui": {
        "task": {
            "ui:help": "Select an evaluation metric from the drop-down list"
        },
        "dataset_path": {
            "ui:widget": "file",
            "ui:help": "Enter the local path to the dataset file. Ensure that this is a csv file with columns: 'input', 'output', 'context'. The context column is optional if using metrics which don't require it."
        }
    }
}