{
    "name": "DeepEval Evaluations (LLM-as-Judge)",
    "uniqueId": "deepeval_llm_judge",
    "description": "Using LLMs as Judges for evaluating outputs of other LLMs",
    "plugin-format": "python",
    "type": "evaluator",
    "version": "0.1.0",
    "git": "https://github.com/confident-ai/deepeval",
    "url": "https://github.com/confident-ai/deepeval",
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": {
        "task": {
            "title": "Evaluation Metric",
            "type": "string",
            "enum": [
                "Bias",
                "Toxicity",
                "FaithFulness",
                "Hallucination",
                "Answer Relevancy",
                "Contextual Precision",
                "Contextual Recall",
                "Contextual Relevancy",
                "Custom (GEval)"
            ]
        },
        "dataset_path": {
            "title": "Dataset Path",
            "type": "string"
        },
        "geval_name": {
            "title": "Criteria Name (Only for GEval)",
            "type": "string"
        },
        "geval_context": {
            "title": "Criteria Description (Only for GEval)",
            "type": "string"
        },
        "context_required": {
            "title": "Should `context` field be considered in dataset? (Only for GEval)",
            "type": "boolean",
            "default": false,
            "required": true
        }
    },
    "parameters-ui": {
        "task": {
            "ui:help": "Select an evaluation metric from the drop-down list"
        },
        "dataset_path": {
            "ui:help": "Enter the local path to the dataset file. Ensure that this is a csv file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it."
        }
    }
}