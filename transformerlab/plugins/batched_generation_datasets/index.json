{
    "name": "Batched Output Generation for Synthetic Dataset",
    "uniqueId": "batched_generation_datasets",
    "description": "Use a local or commercial LLM to generated outputs for the dataset generated.",
    "plugin-format": "python",
    "type": "generator",
    "version": "0.1.1",
    "git": "",
    "url": "",
    "files": [
        "main.py",
        "setup.sh"
    ],
    "_dataset": true,
    "_dataset_display_message": "Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.",
    "setup-script": "setup.sh",
    "parameters": {
        "generation_model": {
            "title": "Generation Model (Model to be used for Generation. Select `local` to use the local model running)",
            "type": "string",
            "enum": [
                "Local",
                "Claude 3.5 Haiku",
                "Claude 3.5 Sonnet",
                "OpenAI GPT 4o",
                "OpenAI GPT 4o Mini"

            ],
            "default": "Local"
        },
        "system_prompt": {
            "title": "System Prompt",
            "type": "string"
        },
        "input_column": {
            "title": "Input Column",
            "type": "string"
        },
        "output_column": {
            "title": "Output Column",
            "type": "string"
        },
        "batch_size": {
            "title": "Batch Size",
            "type": "integer",
            "default": 128
        },
        "temperature": {
            "title": "Temperature",
            "type": "number",
            "default": 0.01,
            "minimum": 0.0,
            "maximum": 2.0,
            "multipleOf": 0.01
        },
        "top_p": {
            "title": "Top P",
            "type": "number",
            "default": 1.0,
            "minimum": 0.0,
            "maximum": 1.0,
            "multipleOf": 0.1
        },
        "max_tokens": {
            "title": "Max Tokens",
            "type": "integer",
            "default": 1024,
            "minimum": 1,
            "maximum": 2048,
            "multipleOf": 1
        }
    },
    "parameters_ui": {
        "generation_model": {
            "ui:help": "Select the model to be used for generation from the drop-down list"
        },
        "input_column": {
            "ui:help": "Select the column from the dataset to be used as input for generation"
        },
        "output_column": {
            "ui:help": "Select the column from the dataset to be used as output for generation"
        },
        "batch_size": {
            "ui:help": "Select the batch size for sending the data to the model for generation"
        },
        "system_prompt": {
            "ui:help": "Enter the system prompt to be used for generation"
        },
        "temperature": {
            "ui:help": "Select the temperature for generation",
            "ui:widget": "RangeWidget"
        },
        "max_tokens": {
            "ui:help": "Select the maximum tokens for generation",
            "ui:widget": "RangeWidget"
        }
    }
}