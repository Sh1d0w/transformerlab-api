{
  "name": "Apple MLX LoRA Trainer",
  "uniqueId": "mlx_lora_trainer",
  "description": "MLX Machine learning research on your laptop or in a data center - by Apple",
  "plugin-format": "python",
  "type": "trainer",
  "version": "0.4.2",
  "model_architectures": [
    "MLX",
    "CohereForCausalLM",
    "DeepseekV2ForCausalLM",
    "GemmaForCausalLM",
    "Gemma2ForCausalLM",
    "GPTBigCodeForCausalLM",
    "LlamaForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "PhiForCausalLM",
    "Phi3ForCausalLM",
    "Qwen2ForCausalLM"
  ],
  "files": [
    "main.py",
    "setup.sh"
  ],
  "setup-script": "setup.sh",
  "parameters": {
    "lora_layers": {
      "title": "LoRA Layers",
      "type": "integer",
      "default": 16,
      "minimum": 4,
      "maximum": 64
    },
    "batch_size": {
      "title": "Batch Size",
      "type": "integer",
      "default": 4,
      "minimum": 1,
      "maximum": 8
    },
    "learning_rate": {
      "title": "Learning Rate",
      "type": "number",
      "default": 5e-5,
      "minimum": 1e-6,
      "maximum": 1e6
    },
    "lora_rank": {
      "title": "LoRA Rank",
      "type": "integer",
      "default": 8,
      "minimum": 1,
      "maximum": 256
    },
    "lora_alpha": {
      "title": "LoRA Alpha",
      "type": "integer",
      "default": 16,
      "minimum": 1,
      "maximum": 256
    },
    "iters": {
      "title": "Iterations",
      "type": "integer",
      "default": 1000,
      "minimum": 1,
      "maximum": 1000000
    },
    "steps_per_report": {
      "title": "Steps per Report",
      "type": "integer",
      "default": 100,
      "minimum": 1
    },
    "steps_per_eval": {
      "title": "Steps per Evaluation",
      "type": "integer",
      "default": 200,
      "minimum": 1
    },
    "save_every": {
      "title": "Save Every",
      "type": "integer",
      "default": 100,
      "minimum": 1
    },
    "adaptor_name": {
      "title": "Adaptor Name",
      "type": "string",
      "default": "adaptor",
      "required": true
    },
    "fuse_model": {
      "title": "Fuse Model after Training",
      "type": "boolean",
      "default": true,
      "required": true
    }
  },
  "parameters_ui": {
    "lora_layers": {
      "ui:help": "Default fine-tune layers is 16. Setting to 8 or 4 reduces memory needed for back propagation, but may reduce the quality if you are tuning with a lot of data."
    },
    "batch_size": {
      "ui:help": "Default batch is 4. Setting this to 2 or 1 will reduce memory consumption but may slow performance."
    },
    "learning_rate": {
      "ui:help": "Adam Learning rate."
    },
    "lora_rank": {
      "ui:help": "Number of trainable parameters to use during training. Default is 8. Increasing can improve learning new concepts but will use more RAM."
    },
    "lora_alpha": {
      "ui:help": "Scaling factor for learned weights. Default is 16. Increasing the ratio of alpha/rank will cause new weights to carry more influence."
    },
    "iters": {
      "ui:help": "Number of iterations (not epochs) to train -- 1000 could be a starting point."
    },
    "steps_per_report": {
      "ui:help": "Number of training steps between loss reporting."
    },
    "steps_per_eval": {
      "ui:help": "Number of training steps between validations."
    },
    "fuse_model": {
      "ui:help": "Fuse adapter and model together. If unchecked, this will create a separate adapter which can be loaded from within the model in Foundation tab."
    }
  }
}