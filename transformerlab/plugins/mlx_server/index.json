{
  "name": "Apple MLX Server",
  "uniqueId": "mlx_server",
  "description": "MLX Machine learning research on your laptop or in a data center - by Apple",
  "plugin-format": "python",
  "type": "loader",
  "version": "0.1.26",
  "model_architectures": [
    "MLX",
    "CohereForCausalLM",
    "DeepseekV2ForCausalLM",
    "GemmaForCausalLM",
    "Gemma2ForCausalLM",
    "Gemma3ForCausalLM",
    "GPTBigCodeForCausalLM",
    "LlamaForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "PhiForCausalLM",
    "Phi3ForCausalLM",
    "Qwen2ForCausalLM"
  ],
  "supported_hardware_architectures": [
    "mlx"
  ],
  "files": [
    "main.py",
    "setup.sh"
  ],
  "setup-script": "setup.sh",
  "parameters": {
    "use_airllm" : {
      "title": "Use AirLLM for Inference",
      "type": "boolean",
      "default": false
    }
},
"parameters_ui": {
  "use_airllm": {
    "ui:help": "Use AirLLM for Inference. Note that enabling this option will make a bigger model run on your system but the inference speed might reduce"
  }
}
}