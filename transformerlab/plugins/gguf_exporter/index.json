{
    "name": "GGUF Exporter",
    "uniqueId": "gguf_exporter",
    "description": "Exports the current model to GGUF format so it can be run on computers without a GPU.",
    "plugin-format": "python",
    "type": "exporter",
    "version": "0.2.1",
    "model_architectures": [
        "CohereForCausalLM",
        "FalconForCausalLM",
        "LlamaForCausalLM",
        "GemmaForCausalLM",
        "Gemma2ForCausalLM",
        "GPTJForCausalLM",
        "MistralForCausalLM",
        "MixtralForCausalLM",
        "PhiForCausalLM",
        "Phi3ForCausalLM",
        "Qwen2ForCausalLM"
    ],
    "supported_hardware_architectures": [
    "cpu",
    "cuda",
    "mlx"
    ],
    "export_architecture": "GGUF",
    "files": [
        "main.py",
        "setup.sh"
    ],
    "setup-script": "setup.sh",
    "parameters": {
        "outtype": {
            "title": "Output Format",
            "type": "string",
            "default": "q8_0",
            "enum": [
                "q8_0",
                "f16",
                "f32"
            ]
        }
    }
}