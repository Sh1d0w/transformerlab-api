{
    "name": "Basic Evaluation Metrics",
    "uniqueId": "basic_evals",
    "description": "Evaluating outputs of LLMs using objective metrics",
    "plugin-format": "python",
    "type": "evaluator",
    "version": "0.1.0",
    "git": "https://github.com/confident-ai/deepeval",
    "url": "https://github.com/confident-ai/deepeval",
    "files": [
        "main.py",
        "setup.sh"
    ],
    "_dataset": true,
    "_dataset_display_message": "Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.",
    "setup-script": "setup.sh",
    "parameters": {
        "tasks": {
            "type": "array",
            "title": "Evaluation Metrics",
            "items": {
                "type": "object",
                "properties": {
                    "name": {
                        "type": "string",
                        "title": "Evaluation Name"
                    },
                    "regex": {
                        "type": "string",
                        "title": "Regular Expression"
                    },
                    "output_type": {
                        "type": "string",
                        "title": "Output Type",
                        "enum": ["boolean", "number"]
                    }
                }
            }
        },
        "limit": {
            "title": "Fraction of samples to evaluate",
            "type": [
              "number"
            ],
            "minimum": 0.0,
            "default": 1.0,
            "maximum": 1.0,
            "multipleOf": 0.1
          }
    },
    "parameters_ui": {
        "limit": {
            "ui:help": "Enter a fraction of samples to evaluate from your data. Set as 1 to get all samples",
            "ui:widget": "RangeWidget"
        },
        "tasks": {
            "ui:help": "Select an evaluation metric from the drop-down list",
            "ui:widget": "EvaluationWidget"
        }
    }
}