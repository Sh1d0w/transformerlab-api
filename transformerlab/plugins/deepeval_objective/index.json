{
    "name": "DeepEval Objective Metrics",
    "uniqueId": "deepeval_objective",
    "description": "Evaluating outputs of LLMs using objective metrics",
    "plugin-format": "python",
    "type": "evaluator",
    "version": "0.1.6",
    "git": "https://github.com/confident-ai/deepeval",
    "url": "https://github.com/confident-ai/deepeval",
    "files": [
        "main.py",
        "setup.sh"
    ],
    "dataset": true,
    "dataset_display_message": "Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.",
    "setup-script": "setup.sh",
    "parameters": {
        "metrics": {
            "title": "Evaluation Metric",
            "type": "string",
            "enum": [
                "Rouge",
                "BLEU",
                "Exact Match",
                "Quasi Exact Match",
                "Quasi Contains",
                "BERT Score"
            ]
        }
    },
    "parameters-ui": {
        "metrics": {
            "ui:help": "Select an evaluation metric from the drop-down list"
        }
    }
}