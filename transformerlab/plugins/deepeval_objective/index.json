{
    "name": "DeepEval Objective Metrics",
    "uniqueId": "deepeval_objective",
    "description": "Evaluating outputs of LLMs using objective metrics",
    "plugin-format": "python",
    "type": "evaluator",
    "version": "0.1.7",
    "git": "https://github.com/confident-ai/deepeval",
    "url": "https://github.com/confident-ai/deepeval",
    "files": [
        "main.py",
        "setup.sh"
    ],
    "dataset": true,
    "dataset_display_message": "Please upload a dataset file with columns: 'input', 'output', 'expected_output'. The context column is optional if using metrics which don't require it.",
    "setup-script": "setup.sh",
    "parameters": {
        "metrics": {
            "title": "Evaluation Metric",
            "type": "string",
            "enum": [
                "Rouge",
                "BLEU",
                "Exact Match",
                "Quasi Exact Match",
                "Quasi Contains",
                "BERT Score"
            ]
        },
        "limit": {
            "title": "Fraction of samples (Enter a floating point between 0 and 1. Set as 1 to get all samples)",
            "type": [
              "number"
            ],
            "minimum": 0.0,
            "default": 1.0,
            "maximum": 1.0
          }
    },
    "parameters-ui": {
        "metrics": {
            "ui:help": "Select an evaluation metric from the drop-down list"
        }
    }
}