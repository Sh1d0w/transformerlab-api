{
  "name": "Fastchat Server",
  "uniqueId": "fastchat_server",
  "description": "Fastchat loads models for inference using Huggingface Transformers for generation.",
  "plugin-format": "python",
  "type": "loader",
  "version": "1.0.4",
  "model_architectures": [
    "CohereForCausalLM",
    "FalconForCausalLM",
    "GemmaForCausalLM",
    "Gemma2ForCausalLM",
    "GPTBigCodeForCausalLM",
    "LlamaForCausalLM",
    "MistralForCausalLM",
    "MixtralForCausalLM",
    "PhiForCausalLM",
    "Phi3ForCausalLM",
    "Qwen2ForCausalLM",
    "T5ForConditionalGeneration"
  ],
  "supported_hardware_architectures": ["cpu", "cuda", "mlx"],
  "files": ["main.py", "setup.sh"],
  "setup-script": "setup.sh",
  "parameters": {
  "gpu_ids": {
          "title": "GPU IDs to use for Inference",
          "type": "string",
          "default": "0"
      },
    "eight_bit": {
      "title": "Enable 8-bit compression",
      "type": "boolean",
      "default": false
    }
  },
  "parameters_ui": {
    "gpu_ids": {
      "ui:help": "Used to spread models over multiple GPUs. Specify a comma-separated list of GPU IDs to use for inference. For example: 0,1,2,3"
    }
  }
}
