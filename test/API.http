

## Test Logprobs:
POST http://localhost:8000/v1/completions
Content-Type: application/json

{
  "model": "Llama-3.2-1B-Instruct-4bit",
  "prompt": "Translate the ",
  "temperature": 0,
  "n": 1,
    "logprobs": 1,
    "echo": true,
    "max_tokens": 8,
    "top_p": 0.95
}

###
## Test batched prompt creation with objects
POST http://localhost:8000/batched_prompts/new
Content-Type: application/json

{
  "name": "test",
  "prompts": [
    {"a": "b"},
    {"c": "d", "e": "f"}
  ]
}

###
## test with string prompts
POST http://localhost:8000/batched_prompts/new
Content-Type: application/json

{
  "name": "name",
  "prompts": [
    "a string",
    "another string"
  ]
}

###
POST http://localhost:8000/batched_prompts/new
Content-Type: application/json

{
  "name":"test3",
  "prompts":
  [
    [{"role":"system","content":"You are a helpful assistant."},{"role":"human","content":"Hello"}],
    [{"role":"system","content":"You are a helpful assistant."},{"role":"human","content":"Hello"}],
    [{"role":"system","content":"You are a helpful assistant."},{"role":"human","content":"Hello"}]
  ]
}
###
## Test batch request mode on completions endpoint
POST http://localhost:8000/v1/completions
Content-Type: application/json

{
  "model": "Llama-3.2-1B-Instruct-4bit",
  "prompt": [
    "Translate the following English sentence to French: 'Hello, how are you?'",
    "How are our customers doing today?"
  ],
    "max_tokens": 60
}

###
## Test sending multiple chats -- note that this doesn't work!
POST http://localhost:8000/v1/chat/completions
Content-Type: application/json

{
  "model": "Llama-3.2-1B-Instruct-4bit",
  "messages":   [
    [{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello"}],
    [{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Bye"}]
  ],
  "max_tokens": 60
}

###
## Test sending a single chat
POST http://localhost:8000/v1/chat/completions
Content-Type: application/json

{
  "model": "Llama-3.2-1B-Instruct-4bit",
  "messages":   
    [{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Hello"}],
  "max_tokens": 60
}

###
## Data: request a preview of a dataset
GET http://localhost:8000/data/preview?dataset_id=samsum&offset=1&limit=10

###
## Data: request a preview of a dataset with a split
GET http://localhost:8000/data/preview?dataset_id=samsum&offset=1&limit=10&split=validation
